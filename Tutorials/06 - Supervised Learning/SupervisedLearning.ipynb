{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with Scikit Learn\n",
    "\n",
    "### Libraries\n",
    "\n",
    "- [scikit-learn](http://scikit-learn.org/stable/)\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "In this tutorial we will learn how to use scikit-learn to train Linear Regression, K-NN, Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, auc, roc_curve\n",
    "# import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Predicting sales from ads expense\n",
    "\n",
    "<img src=\"img/ada_ads.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Advertising.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **features**?\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the **response**?\n",
    "- Sales: sales of a single product in a given market (in thousands of units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the relationship between the features and the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "data.plot(kind='scatter', x='TV', y='sales', ax=axs[0], figsize=(16, 5), grid=True)\n",
    "data.plot(kind='scatter', x='radio', y='sales', ax=axs[1], grid=True)\n",
    "data.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2], grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/estimating_coefficients.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/slope_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands on!\n",
    "Let's create the feature matrix and the class vector (X and y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['TV', 'radio', 'newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.sales\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn** provides an easy way to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()  # create the model\n",
    "lin_reg.fit(X, y)  # train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the theory! Let's see what the formula looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(feature_cols)):\n",
    "    print(\"{0} * {1} + \".format(lin_reg.coef_[f], feature_cols[f]))\n",
    "print(lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1  \\times TV + \\beta_2  \\times radio + \\beta_3  \\times newspaper$$\n",
    "$$y = 2.938 + 0.045 \\times TV + 0.18  \\times radio + -0.001  \\times newspaper$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the predictions and the original values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation? What is that?\n",
    "https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, we have only 3 features, but 200 records, which is enough for learning a good linear model. But what if we had much fewer records, say, 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:5]\n",
    "y = y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: The model remembers the training records (overfitting).\n",
    "\n",
    "**Solution**: Regularization\n",
    "\n",
    "Regularization refers to methods that help to reduce overfitting. Let's try Ridge Regression, which puts a penalty on large weights $\\beta_i$ and forces them to be smaller in magnitude. This reduces the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=6)\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted_r = cross_val_predict(ridge, X, y, cv=5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted_r, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, predicted_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Predicting Titanic survival with Logistic Regression\n",
    "\n",
    "Let's use the data obtained by the _Encyclopedia Titanica_ to predict if a passenger survived the Titanic disaster.\n",
    "\n",
    "<img src=\"img/titanic.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_excel('data/titanic.xls')\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **features**?\n",
    "- name: Name of the passenger\n",
    "- sex: Male or Female\n",
    "- age: Age in years\n",
    "- sibsp: # of siblings / spouses aboard the Titanic\n",
    "- parch: # of parents / children aboard the Titanic\n",
    "- ticket: Ticket number\n",
    "- fare: Ticket price\n",
    "- cabin: Cabin number\n",
    "- embarked: Port of Embarkation\n",
    "\n",
    "What is the **response**?\n",
    "- survived: whether the passenger survived the disaster or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = titanic[titanic['survived']==0]\n",
    "survived = titanic[titanic['survived']==1]\n",
    "\n",
    "print(\"Survived {0}, Dead {1}\".format(len(survived), len(dead)))\n",
    "print(\"Survived {:.2%}\".format(len(survived)/len(titanic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the columns to use as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features = ['sex', 'age', 'sibsp', 'parch', 'fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this execise, we can assume the other features (name, cabin number, embarked) are not predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the feature vector for the training\n",
    "\n",
    "The dataset contains one categorical variable: sex (male|female)\n",
    "\n",
    "We need to convert it to a numerical variable. Pandas offers the method *get_dummies* that takes care of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features vector\n",
    "X = pd.get_dummies(titanic[titanic_features])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical feature _sex_ is converted in 2 boolean features.\n",
    "\n",
    "Titanic sank in 1912: it was a lot of time ago! Some data may be missing. Let's check if there are undefined values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[X.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fix the data with a basic imputation method: replacing the missing values with the mean.\n",
    "\n",
    "More info: https://en.wikipedia.org/wiki/Imputation_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(X.mean())\n",
    "\n",
    "len(X[X.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label used for the traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and evaluate the precison/recall with a cross validation (10 splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "# Precision: avoid false positives\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\n",
    "# Recall: avoid false negatives\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression uses a threshold on the probability to decide at which class to assign a prediction. In some cases, we are interested to understand how the model behaves at different levels of this threshold.\n",
    "\n",
    "Let's give a look at the ROC curve!\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities with a cross validationn\n",
    "y_pred = cross_val_predict(logistic, X, y, cv=10, method=\"predict_proba\")\n",
    "# Compute the False Positive Rate and True Positive Rate\n",
    "fpr, tpr, _ = roc_curve(y, y_pred[:, 1])\n",
    "# Compute the area under the fpt-tpf curve\n",
    "auc_score = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Area = {:.5f}\".format(auc_score));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the model output\n",
    "\n",
    "Let's train on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, since we trained the whole dataset, we don't have new samples to predict, but we can predict the outcome and the relative probability for some artificial samples. Would you have survived?\n",
    "\n",
    "Remember the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would a man, 25 years old without relative onboard, and with a fare of 100 survive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability distribution behind this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a woman, 35 years old, alone onboard and with the same fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# Overview on K-NN\n",
    "\n",
    "Let's create some complex shapes to observe how K-NN behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_gaussian_quantiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_moons, y_moons = make_moons(500, noise=0.2, random_state=0)\n",
    "X_circles, y_circles = make_gaussian_quantiles(n_samples=100, random_state=0)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9,4))\n",
    "\n",
    "axs[0].scatter(X_moons[:,0], X_moons[:,1], c=y_moons)\n",
    "axs[0].set_title('Moon Data')\n",
    "\n",
    "axs[1].scatter(X_circles[:,0], X_circles[:,1], c=y_circles)\n",
    "axs[1].set_title('Circles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting function to predict the class of different areas of the features space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting helper \n",
    "# Source: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_prediction(model, X, y, ax, K):\n",
    "    # step size in the mesh\n",
    "    h = .02\n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "    cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_title(\"K = {}\".format(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the models for both dataset with k=1 and K=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 1\n",
    "clf_moons_1 = KNeighborsClassifier(1)\n",
    "clf_moons_1.fit(X_moons, y_moons)\n",
    "clf_circles_1 = KNeighborsClassifier(1)\n",
    "clf_circles_1.fit(X_circles, y_circles)\n",
    "\n",
    "# K = 15\n",
    "clf_moons_15 = KNeighborsClassifier(15)\n",
    "clf_moons_15.fit(X_moons, y_moons)\n",
    "clf_circles_15 = KNeighborsClassifier(15)\n",
    "clf_circles_15.fit(X_circles, y_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12,9))\n",
    "\n",
    "plot_prediction(clf_moons_1, X_moons, y_moons, axs[0][0], 1)\n",
    "plot_prediction(clf_circles_1, X_circles, y_circles, axs[0][1], 1)\n",
    "\n",
    "plot_prediction(clf_moons_15, X_moons, y_moons, axs[1][0], 15)\n",
    "plot_prediction(clf_circles_15, X_circles, y_circles, axs[1][1], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Exercises\n",
    "\n",
    "### Question 1: Complex ML models...\n",
    " - a) tend to have high bias and low variance\n",
    " - b) are always interpretable\n",
    " - c) are prone to overfitting\n",
    " - d) are prone to underfitting\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Which of the following classification models fulfills all three characteristics: i) it is the quickest to train, ii) it is able to handle complex decision boundaries, and iii) it doesn’t require additional retraining to make predictions that take into account freshly obtained data points?\n",
    "\n",
    " - a) logistic regression\n",
    " - b) k nearest neighbors\n",
    " - c) random forest\n",
    " - d) deep neural network\n",
    " \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: A logistic regression model...\n",
    "\n",
    " - a) is trained by minimizing the least-squares error\n",
    " - b) makes predictions in the range [0,1]\n",
    " - c) makes predictions in the range [0,+inf] \n",
    " - d) Can perfectly separate white from gray dots as a linear function of x = [x1, x2]  when x1, x2 ∈ {0, 1} and y = x1 XOR x2, as in the following picture: \n",
    "\n",
    "<img src=\"img/q4.png\" width=\"250\">\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Train a Random Forest model to predict the if a passenger of Titanic survived.\n",
    "\n",
    "- Use random forest classifier with max tree depth of 3 (and random_state=0)\n",
    "- Train the classifier by variating the number of trees from 1 to 20 (N)\n",
    "- For each step estimate precision/recall with cross validation (10-folds)\n",
    "- Plot 2 curves for different values of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
